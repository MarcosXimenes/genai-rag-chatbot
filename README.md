Of course. Here is a well-crafted README for your project, emphasizing its features, execution, and development nuances, all based on the files you provided.

-----

# GenAI RAG Assistant

This project is a solution to the "Machine Learning Engineering - LLM" challenge. It's a robust system built with Python and FastAPI that allows users to upload PDF documents, which are then processed and indexed for a Retrieval-Augmented Generation (RAG) question-and-answering system. The application leverages Google Cloud's Vertex AI for generating embeddings and Firestore for storing them, providing a scalable and efficient backend for contextual AI-powered chats.

## ‚ú® Core Features

  * **PDF Document Ingestion**: Upload one or more PDF documents through a REST API.
  * **Text Extraction and Chunking**: Automatically extracts text from PDFs and splits it into manageable chunks for accurate embedding.
  * **Vector Embeddings**: Generates vector embeddings for each text chunk using Google's Gemini models via Vertex AI.
  * **Efficient Vector Storage**: Stores text chunks and their corresponding vectors in Google Firestore for efficient semantic retrieval.
  * **Contextual Q\&A**: Ask questions about the document content and receive accurate, context-aware answers generated by an LLM.
  * **Source Referencing**: Responses include the source text snippets from the original documents that were used to generate the answer.
  * **Dual LLM Fallback**: Implements a fallback mechanism, first attempting to generate answers with Google's Gemini Pro and switching to OpenAI's GPT model if the primary model fails.
  * **Developer-Friendly**: Includes a `Makefile` for easy setup and execution, and a `Dockerfile` for containerization.

-----

## üõ†Ô∏è Tech Stack & Architecture

This project is built with a modern, cloud-native architecture designed for scalability and maintainability.

  * **Backend Framework**: **FastAPI** for building a high-performance, asynchronous API.
  * **Cloud Provider**: **Google Cloud Platform (GCP)**.
  * **AI & Embeddings**: **Google Cloud Vertex AI** is used to access Gemini models for text embeddings (`gemini-embedding-001`).
  * **Vector Database**: **Google Firestore** is used in place of a traditional vector database. It stores the text chunks and their associated vector embeddings, which are retrieved and used to build a vector store in memory at query time.
  * **LLM Orchestration**: **LangChain** is used to structure the RAG pipeline, manage prompts, and interact with different LLM providers.
  * [cite\_start]**Containerization**: **Docker** is used to create a consistent and isolated environment for the application. [cite: 15]
  * [cite\_start]**Developer Tooling**: A `Makefile` provides convenient commands for installation, environment setup, and running the application. [cite: 1]

-----

## üöÄ Getting Started

Follow these steps to set up and run the project on your local machine.

### Prerequisites

  * Python 3.11
  * A Google Cloud Platform project
  * Google Cloud credentials (`gcloud auth application-default login`)
  * An OpenAI API key (optional, for the fallback mechanism)

### 1\. Environment Setup

The project requires a `.env` file with your specific GCP and API key configurations. A template is provided for convenience.

[cite\_start]First, run the `setup-env` command to create an `.env.example` file[cite: 4]:

```sh
make setup-env
```

Next, copy the example file to a new `.env` file and populate it with your credentials and project details:

```sh
cp .env.example .env
```

Your `.env` file will look like this. Fill in the `your-*` values:

```env
## ‚öôÔ∏è General Project Settings
PROJECT_ID=your-gcp-project-id
SERVICE=genai-rag-assistant

## ‚òÅÔ∏è Google Cloud & Vertex AI Settings
GOOGLE_CLOUD_PROJECT=your-gcp-project-id
GOOGLE_CLOUD_LOCATION=us-central1
GOOGLE_GENAI_USE_VERTEXAI=True
GOOGLE_APPLICATION_CREDENTIALS=./credentials/credential.json

## ü§ñ AI Model Settings
GEMINI_EMBEDDING_MODEL=gemini-embedding-001
GEMINI_EMBEDDING_DIMENSIONALITY=2048
GEMINI_QA_MODEL=gemini-2.5-pro
OPENAI_API_KEY=sk-your-openai-api-key
OPENAI_QA_MODEL=gpt-4o
```

### üîë Google Cloud Permissions (IAM)

For the application to function correctly in the Google Cloud environment, it is crucial to create a **Service Account** with the appropriate permissions (IAM Roles). This account will be used by the application to securely authenticate and interact with GCP services.

Create a new service account in your GCP project and grant it the following permissions:

* **Vertex AI User** (`roles/aiplatform.user`): Allows the application to access Vertex AI models to generate embeddings and responses.
* **Cloud Datastore User** (`roles/datastore.user`): Grants read and write permissions to Firestore, which is used as our database to store text chunks and their embedding vectors.
* **Cloud Trace Agent** (`roles/cloudtrace.agent`): Required for the application to send telemetry data (traces) to Google Cloud Trace, aiding in monitoring and debugging.
* **Error Reporting Writer** (`roles/errorreporting.writer`): Allows the application to send error logs to GCP's Error Reporting service.
* **Service Account Token Creator** (`roles/iam.serviceAccountTokenCreator`): Allows the service account to create access tokens to securely authenticate with other Google APIs.

After creating the service account and assigning these permissions, you must generate an access key in **JSON** format. Rename the key file to `credential.json` and place it in the `./credentials/` directory. [cite_start]This path is referenced in the `GOOGLE_APPLICATION_CREDENTIALS` environment variable[cite: 15, 4].

### 2\. Installation

[cite\_start]The `install` command creates a Python virtual environment, activates it, and installs all the required dependencies from `requirements.txt`. [cite: 2, 3]

```sh
make install
```

### 3\. Running the Development Server

To start the application, use the `run-dev` command. [cite\_start]This command first checks if the `.env` file exists and then launches a Uvicorn server with hot-reloading enabled on port 8080. [cite: 5, 6, 8, 9]

```sh
make run-dev
```

[cite\_start]If the `.env` file is not found, the command will exit with an error, prompting you to create it first. [cite: 7]

The API documentation (Swagger UI) will be available at `http://localhost:8080/docs`.

-----

## üî¨ Development Nuances & Insights

This project demonstrates several key concepts in building modern LLM-powered applications.

### "Root" RAG Implementation

Instead of relying on a managed vector database service, this system implements a "raw" or "from-scratch" approach to RAG. Here‚Äôs how it works:

1.  **Ingestion**: When a PDF is uploaded, it's chunked, and embeddings are generated via Vertex AI.
2.  **Storage**: The text chunks and their corresponding `Vector` embeddings are stored directly in Firestore documents.
3.  **Retrieval**: When a question is asked, the application fetches all vectors and texts for the current user session from Firestore.
4.  **In-Memory Search**: It then uses `FAISS` (Facebook AI Similarity Search) from the LangChain library to build an in-memory vector store on-the-fly. This store is used to find the most relevant text chunks related to the user's question.

This approach provides maximum control over the retrieval process and leverages a common NoSQL database (Firestore) for vector storage, which can be cost-effective for smaller-scale applications.

### Resilient Model Fallback

The system is designed to be resilient. When generating an answer, it first attempts to use the `Gemini` model from Google. If this API call fails for any reason (e.g., rate limits, server errors, content filtering), the system automatically retries the request using the `GPT-4o` model from OpenAI as a fallback. This ensures a higher availability for the Q\&A functionality.

### Cloud-Native and Asynchronous by Design

The application is built to be cloud-native and highly efficient:

  * **Google Cloud Integration**: It deeply integrates with GCP services like Vertex AI for embeddings and Firestore for data persistence.
  * **Asynchronous Processing**: The use of `asyncio` and FastAPI allows for concurrent handling of multiple file uploads and API requests. For instance, multiple PDFs can be processed in parallel, significantly reducing the total ingestion time.

-----

## üì¶ API Endpoints

The API is structured for clarity and ease of use.

### Document Indexing

  * **Endpoint**: `POST /api/v1/document/index`

  * **Description**: Uploads one or more PDF documents to be processed and indexed.

  * **Request (`multipart/form-data`)**:

      * `user`: (string) A unique identifier for the user.
      * `session`: (string) A unique identifier for the chat session.
      * `files`: One or more PDF files.

  * **Success Response (`200 OK`)**:

    ```json
    {
      "message": "Document processing completed.",
      "results": {
        "successful": [
          {
            "filename": "my_document.pdf",
            "status": "success",
            "indexed_chunks": 64,
            "document_ids": ["..."]
          }
        ],
        "failed": []
      }
    }
    ```

### Question Answering

  * **Endpoint**: `POST /api/v1/chat/question`

  * **Description**: Submits a question about the indexed documents for a specific user and session.

  * **Request (`application/json`)**:

      * `user`: (string) The user identifier.
      * `session`: (string) The session identifier.
      * `question`: (string) The question you want to ask.

    <!-- end list -->

    ```json
    {
      "user": "user-123",
      "session": "session-abc",
      "question": "What is the power consumption of the motor?"
    }
    ```

  * **Success Response (`200 OK`)**:

    ```json
    {
      "data": {
        "answer": "The motor's power consumption is 2.3 kW.",
        "sources": [
          {
            "page_content": "the motor xxx has requires 2.3kw to operate at a 60hz line frequency",
            "metadata": {}
          }
        ],
        "model_used": "Gemini"
      }
    }
    ```

-----

## üßπ Housekeeping Commands

The `Makefile` includes additional commands to help with development:

  * [cite\_start]**Code Formatting**: Formats the codebase using `isort`, `black`, and `ruff`. [cite: 10, 11]

    ```sh
    make code-formatting
    ```

  * [cite\_start]**Clean Up**: Removes Python cache files and other temporary artifacts. [cite: 12, 13, 14]

    ```sh
    make clear
    ```